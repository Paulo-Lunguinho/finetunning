{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "with open(filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"JSON carregado! Total de itens: {len(data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "7XAnklyEwMsm",
        "outputId": "cd802170-84a3-4445-c804-91c2f6cc8720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-67858ec5-f765-47af-9832-0f70e82e51fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-67858ec5-f765-47af-9832-0f70e82e51fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_small.json to data_small (2).json\n",
            "JSON carregado! Total de itens: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Formatação das entradas"
      ],
      "metadata": {
        "id": "dOG-B6upwxju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_uJrJlrk--D"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "    desired_response = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "    return instruction_text + input_text + desired_response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_input(data[50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vD7GA0kvypX",
        "outputId": "58c2a2b6-3f1d-437c-cb16-20a11f9fa99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the question based on the context provided below. \"Please provide information on which organization gave a prestigious endorsement to the First Year of Studies program at Notre Dame University.\"\n",
            "\n",
            "### Input:\n",
            "All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\n",
            "\n",
            "### Response:\n",
            "U.S. News & World Report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split"
      ],
      "metadata": {
        "id": "eAdJJprYw2oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.8)\n",
        "test_portion = int(len(data) * 0.1)\n",
        "val_portion = len(data) - train_portion - test_portion\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:(train_portion + test_portion)]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "print(len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUEtdl_Nwo_N",
        "outputId": "79f3a5a7-f080-42da-d416-71732dcc95a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240\n",
            "30\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Dataset"
      ],
      "metadata": {
        "id": "P9pR_d4F4NDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "OvHIZkyc0OSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom Collate Function"
      ],
      "metadata": {
        "id": "inJ6nVjz4QdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "PwOoPI_21RBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_fn(batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGs1NEvI1ZpT",
        "outputId": "bbe2a872-a044-4c7e-e4f6-3ee8fd0cab72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]]), tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Choose Device\n"
      ],
      "metadata": {
        "id": "WFuqngDP4F6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    # Use PyTorch 2.9 or newer for stable mps results\n",
        "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
        "    if (major, minor) >= (2, 9):\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsnwnpIS29WV",
        "outputId": "f3d69c6a-a298-4451-f4bb-47f9848b5751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "H7SYrAGM3h4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DataLoaders"
      ],
      "metadata": {
        "id": "sgRsU4-Z4BcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 1\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "jgFe-z0s3nrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjStogKT4vBy",
        "outputId": "4fe03fdc-8feb-44e2-ee92-4e1fde5b3b11",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([1, 243]) torch.Size([1, 243])\n",
            "torch.Size([1, 93]) torch.Size([1, 93])\n",
            "torch.Size([1, 586]) torch.Size([1, 586])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 896]) torch.Size([1, 896])\n",
            "torch.Size([1, 252]) torch.Size([1, 252])\n",
            "torch.Size([1, 248]) torch.Size([1, 248])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 201]) torch.Size([1, 201])\n",
            "torch.Size([1, 296]) torch.Size([1, 296])\n",
            "torch.Size([1, 111]) torch.Size([1, 111])\n",
            "torch.Size([1, 87]) torch.Size([1, 87])\n",
            "torch.Size([1, 920]) torch.Size([1, 920])\n",
            "torch.Size([1, 160]) torch.Size([1, 160])\n",
            "torch.Size([1, 138]) torch.Size([1, 138])\n",
            "torch.Size([1, 983]) torch.Size([1, 983])\n",
            "torch.Size([1, 110]) torch.Size([1, 110])\n",
            "torch.Size([1, 100]) torch.Size([1, 100])\n",
            "torch.Size([1, 571]) torch.Size([1, 571])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 78]) torch.Size([1, 78])\n",
            "torch.Size([1, 944]) torch.Size([1, 944])\n",
            "torch.Size([1, 90]) torch.Size([1, 90])\n",
            "torch.Size([1, 132]) torch.Size([1, 132])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 100]) torch.Size([1, 100])\n",
            "torch.Size([1, 222]) torch.Size([1, 222])\n",
            "torch.Size([1, 406]) torch.Size([1, 406])\n",
            "torch.Size([1, 114]) torch.Size([1, 114])\n",
            "torch.Size([1, 260]) torch.Size([1, 260])\n",
            "torch.Size([1, 90]) torch.Size([1, 90])\n",
            "torch.Size([1, 315]) torch.Size([1, 315])\n",
            "torch.Size([1, 182]) torch.Size([1, 182])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 914]) torch.Size([1, 914])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 86]) torch.Size([1, 86])\n",
            "torch.Size([1, 229]) torch.Size([1, 229])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 93]) torch.Size([1, 93])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 680]) torch.Size([1, 680])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 97]) torch.Size([1, 97])\n",
            "torch.Size([1, 778]) torch.Size([1, 778])\n",
            "torch.Size([1, 92]) torch.Size([1, 92])\n",
            "torch.Size([1, 992]) torch.Size([1, 992])\n",
            "torch.Size([1, 285]) torch.Size([1, 285])\n",
            "torch.Size([1, 639]) torch.Size([1, 639])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 83]) torch.Size([1, 83])\n",
            "torch.Size([1, 80]) torch.Size([1, 80])\n",
            "torch.Size([1, 108]) torch.Size([1, 108])\n",
            "torch.Size([1, 78]) torch.Size([1, 78])\n",
            "torch.Size([1, 87]) torch.Size([1, 87])\n",
            "torch.Size([1, 95]) torch.Size([1, 95])\n",
            "torch.Size([1, 103]) torch.Size([1, 103])\n",
            "torch.Size([1, 238]) torch.Size([1, 238])\n",
            "torch.Size([1, 191]) torch.Size([1, 191])\n",
            "torch.Size([1, 338]) torch.Size([1, 338])\n",
            "torch.Size([1, 466]) torch.Size([1, 466])\n",
            "torch.Size([1, 340]) torch.Size([1, 340])\n",
            "torch.Size([1, 191]) torch.Size([1, 191])\n",
            "torch.Size([1, 83]) torch.Size([1, 83])\n",
            "torch.Size([1, 89]) torch.Size([1, 89])\n",
            "torch.Size([1, 94]) torch.Size([1, 94])\n",
            "torch.Size([1, 240]) torch.Size([1, 240])\n",
            "torch.Size([1, 459]) torch.Size([1, 459])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 294]) torch.Size([1, 294])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 99]) torch.Size([1, 99])\n",
            "torch.Size([1, 90]) torch.Size([1, 90])\n",
            "torch.Size([1, 79]) torch.Size([1, 79])\n",
            "torch.Size([1, 93]) torch.Size([1, 93])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 94]) torch.Size([1, 94])\n",
            "torch.Size([1, 248]) torch.Size([1, 248])\n",
            "torch.Size([1, 152]) torch.Size([1, 152])\n",
            "torch.Size([1, 153]) torch.Size([1, 153])\n",
            "torch.Size([1, 545]) torch.Size([1, 545])\n",
            "torch.Size([1, 99]) torch.Size([1, 99])\n",
            "torch.Size([1, 248]) torch.Size([1, 248])\n",
            "torch.Size([1, 97]) torch.Size([1, 97])\n",
            "torch.Size([1, 479]) torch.Size([1, 479])\n",
            "torch.Size([1, 83]) torch.Size([1, 83])\n",
            "torch.Size([1, 197]) torch.Size([1, 197])\n",
            "torch.Size([1, 193]) torch.Size([1, 193])\n",
            "torch.Size([1, 949]) torch.Size([1, 949])\n",
            "torch.Size([1, 333]) torch.Size([1, 333])\n",
            "torch.Size([1, 178]) torch.Size([1, 178])\n",
            "torch.Size([1, 94]) torch.Size([1, 94])\n",
            "torch.Size([1, 120]) torch.Size([1, 120])\n",
            "torch.Size([1, 212]) torch.Size([1, 212])\n",
            "torch.Size([1, 433]) torch.Size([1, 433])\n",
            "torch.Size([1, 734]) torch.Size([1, 734])\n",
            "torch.Size([1, 981]) torch.Size([1, 981])\n",
            "torch.Size([1, 342]) torch.Size([1, 342])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 84]) torch.Size([1, 84])\n",
            "torch.Size([1, 86]) torch.Size([1, 86])\n",
            "torch.Size([1, 95]) torch.Size([1, 95])\n",
            "torch.Size([1, 94]) torch.Size([1, 94])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 118]) torch.Size([1, 118])\n",
            "torch.Size([1, 88]) torch.Size([1, 88])\n",
            "torch.Size([1, 407]) torch.Size([1, 407])\n",
            "torch.Size([1, 117]) torch.Size([1, 117])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 451]) torch.Size([1, 451])\n",
            "torch.Size([1, 144]) torch.Size([1, 144])\n",
            "torch.Size([1, 220]) torch.Size([1, 220])\n",
            "torch.Size([1, 112]) torch.Size([1, 112])\n",
            "torch.Size([1, 211]) torch.Size([1, 211])\n",
            "torch.Size([1, 283]) torch.Size([1, 283])\n",
            "torch.Size([1, 344]) torch.Size([1, 344])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 193]) torch.Size([1, 193])\n",
            "torch.Size([1, 457]) torch.Size([1, 457])\n",
            "torch.Size([1, 89]) torch.Size([1, 89])\n",
            "torch.Size([1, 86]) torch.Size([1, 86])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 124]) torch.Size([1, 124])\n",
            "torch.Size([1, 333]) torch.Size([1, 333])\n",
            "torch.Size([1, 84]) torch.Size([1, 84])\n",
            "torch.Size([1, 125]) torch.Size([1, 125])\n",
            "torch.Size([1, 96]) torch.Size([1, 96])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 226]) torch.Size([1, 226])\n",
            "torch.Size([1, 87]) torch.Size([1, 87])\n",
            "torch.Size([1, 282]) torch.Size([1, 282])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 678]) torch.Size([1, 678])\n",
            "torch.Size([1, 185]) torch.Size([1, 185])\n",
            "torch.Size([1, 701]) torch.Size([1, 701])\n",
            "torch.Size([1, 201]) torch.Size([1, 201])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 345]) torch.Size([1, 345])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 183]) torch.Size([1, 183])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 939]) torch.Size([1, 939])\n",
            "torch.Size([1, 107]) torch.Size([1, 107])\n",
            "torch.Size([1, 333]) torch.Size([1, 333])\n",
            "torch.Size([1, 805]) torch.Size([1, 805])\n",
            "torch.Size([1, 285]) torch.Size([1, 285])\n",
            "torch.Size([1, 96]) torch.Size([1, 96])\n",
            "torch.Size([1, 214]) torch.Size([1, 214])\n",
            "torch.Size([1, 193]) torch.Size([1, 193])\n",
            "torch.Size([1, 250]) torch.Size([1, 250])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 321]) torch.Size([1, 321])\n",
            "torch.Size([1, 127]) torch.Size([1, 127])\n",
            "torch.Size([1, 93]) torch.Size([1, 93])\n",
            "torch.Size([1, 938]) torch.Size([1, 938])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 323]) torch.Size([1, 323])\n",
            "torch.Size([1, 99]) torch.Size([1, 99])\n",
            "torch.Size([1, 95]) torch.Size([1, 95])\n",
            "torch.Size([1, 243]) torch.Size([1, 243])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 81]) torch.Size([1, 81])\n",
            "torch.Size([1, 148]) torch.Size([1, 148])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 96]) torch.Size([1, 96])\n",
            "torch.Size([1, 343]) torch.Size([1, 343])\n",
            "torch.Size([1, 93]) torch.Size([1, 93])\n",
            "torch.Size([1, 86]) torch.Size([1, 86])\n",
            "torch.Size([1, 284]) torch.Size([1, 284])\n",
            "torch.Size([1, 79]) torch.Size([1, 79])\n",
            "torch.Size([1, 617]) torch.Size([1, 617])\n",
            "torch.Size([1, 91]) torch.Size([1, 91])\n",
            "torch.Size([1, 105]) torch.Size([1, 105])\n",
            "torch.Size([1, 94]) torch.Size([1, 94])\n",
            "torch.Size([1, 340]) torch.Size([1, 340])\n",
            "torch.Size([1, 108]) torch.Size([1, 108])\n",
            "torch.Size([1, 967]) torch.Size([1, 967])\n",
            "torch.Size([1, 1024]) torch.Size([1, 1024])\n",
            "torch.Size([1, 97]) torch.Size([1, 97])\n",
            "torch.Size([1, 440]) torch.Size([1, 440])\n",
            "torch.Size([1, 337]) torch.Size([1, 337])\n",
            "torch.Size([1, 203]) torch.Size([1, 203])\n",
            "torch.Size([1, 88]) torch.Size([1, 88])\n",
            "torch.Size([1, 609]) torch.Size([1, 609])\n",
            "torch.Size([1, 107]) torch.Size([1, 107])\n",
            "torch.Size([1, 83]) torch.Size([1, 83])\n",
            "torch.Size([1, 86]) torch.Size([1, 86])\n",
            "torch.Size([1, 102]) torch.Size([1, 102])\n",
            "torch.Size([1, 81]) torch.Size([1, 81])\n",
            "torch.Size([1, 283]) torch.Size([1, 283])\n",
            "torch.Size([1, 106]) torch.Size([1, 106])\n",
            "torch.Size([1, 111]) torch.Size([1, 111])\n",
            "torch.Size([1, 105]) torch.Size([1, 105])\n",
            "torch.Size([1, 167]) torch.Size([1, 167])\n",
            "torch.Size([1, 160]) torch.Size([1, 160])\n",
            "torch.Size([1, 100]) torch.Size([1, 100])\n",
            "torch.Size([1, 132]) torch.Size([1, 132])\n",
            "torch.Size([1, 341]) torch.Size([1, 341])\n",
            "torch.Size([1, 370]) torch.Size([1, 370])\n",
            "torch.Size([1, 211]) torch.Size([1, 211])\n",
            "torch.Size([1, 107]) torch.Size([1, 107])\n",
            "torch.Size([1, 793]) torch.Size([1, 793])\n",
            "torch.Size([1, 106]) torch.Size([1, 106])\n",
            "torch.Size([1, 92]) torch.Size([1, 92])\n",
            "torch.Size([1, 96]) torch.Size([1, 96])\n",
            "torch.Size([1, 144]) torch.Size([1, 144])\n",
            "torch.Size([1, 213]) torch.Size([1, 213])\n",
            "torch.Size([1, 88]) torch.Size([1, 88])\n",
            "torch.Size([1, 102]) torch.Size([1, 102])\n",
            "torch.Size([1, 198]) torch.Size([1, 198])\n",
            "torch.Size([1, 674]) torch.Size([1, 674])\n",
            "torch.Size([1, 108]) torch.Size([1, 108])\n",
            "torch.Size([1, 77]) torch.Size([1, 77])\n",
            "torch.Size([1, 114]) torch.Size([1, 114])\n",
            "torch.Size([1, 865]) torch.Size([1, 865])\n",
            "torch.Size([1, 137]) torch.Size([1, 137])\n",
            "torch.Size([1, 85]) torch.Size([1, 85])\n",
            "torch.Size([1, 119]) torch.Size([1, 119])\n",
            "torch.Size([1, 103]) torch.Size([1, 103])\n",
            "torch.Size([1, 106]) torch.Size([1, 106])\n",
            "torch.Size([1, 88]) torch.Size([1, 88])\n",
            "torch.Size([1, 151]) torch.Size([1, 151])\n",
            "torch.Size([1, 841]) torch.Size([1, 841])\n",
            "torch.Size([1, 826]) torch.Size([1, 826])\n",
            "torch.Size([1, 569]) torch.Size([1, 569])\n",
            "torch.Size([1, 219]) torch.Size([1, 219])\n",
            "torch.Size([1, 92]) torch.Size([1, 92])\n",
            "torch.Size([1, 80]) torch.Size([1, 80])\n",
            "torch.Size([1, 250]) torch.Size([1, 250])\n",
            "torch.Size([1, 199]) torch.Size([1, 199])\n",
            "torch.Size([1, 80]) torch.Size([1, 80])\n",
            "torch.Size([1, 280]) torch.Size([1, 280])\n",
            "torch.Size([1, 126]) torch.Size([1, 126])\n",
            "torch.Size([1, 130]) torch.Size([1, 130])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download function"
      ],
      "metadata": {
        "id": "Me6htB5G5svp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path, backup_url)\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "\n",
        "def download_file(url, destination, backup_url=None):\n",
        "    def _attempt_download(download_url):\n",
        "        response = requests.get(download_url, stream=True, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "        # Check if file exists and has same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size and file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return True\n",
        "\n",
        "        block_size = 1024  # 1 KB\n",
        "        desc = os.path.basename(download_url)\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
        "            with open(destination, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=block_size):\n",
        "                    if chunk:\n",
        "                        file.write(chunk)\n",
        "                        progress_bar.update(len(chunk))\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        if _attempt_download(url):\n",
        "            return\n",
        "    except requests.exceptions.RequestException:\n",
        "        if backup_url is not None:\n",
        "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
        "            try:\n",
        "                if _attempt_download(backup_url):\n",
        "                    return\n",
        "            except requests.exceptions.RequestException:\n",
        "                pass\n",
        "\n",
        "        error_message = (\n",
        "            f\"Failed to download from both primary URL ({url})\"\n",
        "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
        "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
        "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
        "        )\n",
        "        print(error_message)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "LzmwEuFX6Ok4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "5p86FH646vNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "#\n",
        "# This file collects all the relevant code that we covered thus far\n",
        "# throughout Chapters 2-6.\n",
        "# This file can be run as a standalone script.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 2\n",
        "#####################################\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 3\n",
        "#####################################\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 4\n",
        "#####################################\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Chapter 5\n",
        "#####################################\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
        "            # subtract rowwise max before softmax\n",
        "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xZgLnqMV6mZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading Pre-trained Model"
      ],
      "metadata": {
        "id": "if5KY05_6zeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfPBwWVe5wYq",
        "outputId": "75d6125c-c945-41ca-c522-55d2bf4fac6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial Losses"
      ],
      "metadata": {
        "id": "WTYHD6dvAiE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwGyi9zA_Dus",
        "outputId": "4c2b9110-9fbb-4a96-f5ac-03d7cd23add6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.1463036060333254\n",
            "Validation loss: 3.817526388168335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.pos_emb.num_embeddings = 256"
      ],
      "metadata": {
        "id": "4_jZhq62fz4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response1.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing\n",
        "files.download(\"instruction-data-with-response1.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c-_w4nMq8Byb",
        "outputId": "9e46273d-2e18-4a8a-f591-cf2d97788088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [15:09<00:00, 30.31s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc478818-0e09-4acf-b78a-a2d0b386139b\", \"instruction-data-with-response1.json\", 64742)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###New Training"
      ],
      "metadata": {
        "id": "bAwVmkYMAdX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6RD9T-ijejdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u-1KrzQAc0l",
        "outputId": "ab5248b7-1355-4d2b-a339-2cab12ec4710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 3.121, Val loss 3.311\n",
            "Ep 1 (Step 000005): Train loss 2.727, Val loss 2.628\n",
            "Ep 1 (Step 000010): Train loss 2.338, Val loss 2.188\n",
            "Ep 1 (Step 000015): Train loss 2.575, Val loss 1.543\n",
            "Ep 1 (Step 000020): Train loss 1.716, Val loss 1.305\n",
            "Ep 1 (Step 000025): Train loss 1.996, Val loss 1.104\n",
            "Ep 1 (Step 000030): Train loss 1.600, Val loss 1.093\n",
            "Ep 1 (Step 000035): Train loss 1.925, Val loss 1.053\n",
            "Ep 1 (Step 000040): Train loss 1.360, Val loss 1.010\n",
            "Ep 1 (Step 000045): Train loss 1.328, Val loss 1.007\n",
            "Ep 1 (Step 000050): Train loss 1.130, Val loss 0.996\n",
            "Ep 1 (Step 000055): Train loss 1.051, Val loss 0.928\n",
            "Ep 1 (Step 000060): Train loss 2.383, Val loss 0.926\n",
            "Ep 1 (Step 000065): Train loss 1.625, Val loss 0.998\n",
            "Ep 1 (Step 000070): Train loss 1.083, Val loss 0.900\n",
            "Ep 1 (Step 000075): Train loss 1.642, Val loss 0.871\n",
            "Ep 1 (Step 000080): Train loss 0.754, Val loss 0.851\n",
            "Ep 1 (Step 000085): Train loss 2.035, Val loss 0.858\n",
            "Ep 1 (Step 000090): Train loss 1.465, Val loss 0.856\n",
            "Ep 1 (Step 000095): Train loss 1.243, Val loss 0.861\n",
            "Ep 1 (Step 000100): Train loss 1.586, Val loss 0.878\n",
            "Ep 1 (Step 000105): Train loss 1.540, Val loss 0.847\n",
            "Ep 1 (Step 000110): Train loss 1.799, Val loss 0.828\n",
            "Ep 1 (Step 000115): Train loss 0.858, Val loss 0.832\n",
            "Ep 1 (Step 000120): Train loss 1.130, Val loss 0.859\n",
            "Ep 1 (Step 000125): Train loss 0.750, Val loss 0.787\n",
            "Ep 1 (Step 000130): Train loss 0.990, Val loss 0.709\n",
            "Ep 1 (Step 000135): Train loss 0.862, Val loss 0.709\n",
            "Ep 1 (Step 000140): Train loss 1.244, Val loss 0.718\n",
            "Ep 1 (Step 000145): Train loss 1.433, Val loss 0.731\n",
            "Ep 1 (Step 000150): Train loss 0.877, Val loss 0.718\n",
            "Ep 1 (Step 000155): Train loss 0.527, Val loss 0.723\n",
            "Ep 1 (Step 000160): Train loss 1.039, Val loss 0.713\n",
            "Ep 1 (Step 000165): Train loss 0.955, Val loss 0.686\n",
            "Ep 1 (Step 000170): Train loss 0.637, Val loss 0.687\n",
            "Ep 1 (Step 000175): Train loss 0.861, Val loss 0.661\n",
            "Ep 1 (Step 000180): Train loss 1.183, Val loss 0.663\n",
            "Ep 1 (Step 000185): Train loss 0.877, Val loss 0.692\n",
            "Ep 1 (Step 000190): Train loss 0.916, Val loss 0.704\n",
            "Ep 1 (Step 000195): Train loss 0.692, Val loss 0.688\n",
            "Ep 1 (Step 000200): Train loss 0.707, Val loss 0.669\n",
            "Ep 1 (Step 000205): Train loss 1.000, Val loss 0.710\n",
            "Ep 1 (Step 000210): Train loss 0.678, Val loss 0.714\n",
            "Ep 1 (Step 000215): Train loss 0.628, Val loss 0.677\n",
            "Ep 1 (Step 000220): Train loss 1.189, Val loss 0.645\n",
            "Ep 1 (Step 000225): Train loss 0.448, Val loss 0.639\n",
            "Ep 1 (Step 000230): Train loss 1.416, Val loss 0.653\n",
            "Ep 1 (Step 000235): Train loss 1.039, Val loss 0.642\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \"Analyze the input text and categorize its emotional tone as either positive or negative based on the language and context used within the given statement.\"  ### Input: proves once again he has n't lost his touch , bringing off a superb performance in an admittedly middling film .   ### Response: Positive  ### Response: Positive<|endoftext|>The U.S. Department of Justice has filed a lawsuit against the University of Notre Dame in federal court in Chicago, Illinois, alleging that the school \"engaged in a pattern of unlawful discrimination\" against\n",
            "Ep 2 (Step 000240): Train loss 0.370, Val loss 0.632\n",
            "Ep 2 (Step 000245): Train loss 0.926, Val loss 0.652\n",
            "Ep 2 (Step 000250): Train loss 1.453, Val loss 0.650\n",
            "Ep 2 (Step 000255): Train loss 0.696, Val loss 0.671\n",
            "Ep 2 (Step 000260): Train loss 1.299, Val loss 0.651\n",
            "Ep 2 (Step 000265): Train loss 1.238, Val loss 0.630\n",
            "Ep 2 (Step 000270): Train loss 0.915, Val loss 0.664\n",
            "Ep 2 (Step 000275): Train loss 0.594, Val loss 0.655\n",
            "Ep 2 (Step 000280): Train loss 1.250, Val loss 0.650\n",
            "Ep 2 (Step 000285): Train loss 1.234, Val loss 0.643\n",
            "Ep 2 (Step 000290): Train loss 0.830, Val loss 0.632\n",
            "Ep 2 (Step 000295): Train loss 1.004, Val loss 0.636\n",
            "Ep 2 (Step 000300): Train loss 1.220, Val loss 0.652\n",
            "Ep 2 (Step 000305): Train loss 1.077, Val loss 0.668\n",
            "Ep 2 (Step 000310): Train loss 0.636, Val loss 0.666\n",
            "Ep 2 (Step 000315): Train loss 0.773, Val loss 0.672\n",
            "Ep 2 (Step 000320): Train loss 0.818, Val loss 0.679\n",
            "Ep 2 (Step 000325): Train loss 1.016, Val loss 0.672\n",
            "Ep 2 (Step 000330): Train loss 0.493, Val loss 0.670\n",
            "Ep 2 (Step 000335): Train loss 0.676, Val loss 0.659\n",
            "Ep 2 (Step 000340): Train loss 0.761, Val loss 0.660\n",
            "Ep 2 (Step 000345): Train loss 1.215, Val loss 0.662\n",
            "Ep 2 (Step 000350): Train loss 0.521, Val loss 0.662\n",
            "Ep 2 (Step 000355): Train loss 1.235, Val loss 0.681\n",
            "Ep 2 (Step 000360): Train loss 0.312, Val loss 0.704\n",
            "Ep 2 (Step 000365): Train loss 0.643, Val loss 0.718\n",
            "Ep 2 (Step 000370): Train loss 0.537, Val loss 0.725\n",
            "Ep 2 (Step 000375): Train loss 0.750, Val loss 0.711\n",
            "Ep 2 (Step 000380): Train loss 0.565, Val loss 0.690\n",
            "Ep 2 (Step 000385): Train loss 0.983, Val loss 0.686\n",
            "Ep 2 (Step 000390): Train loss 0.617, Val loss 0.685\n",
            "Ep 2 (Step 000395): Train loss 0.964, Val loss 0.677\n",
            "Ep 2 (Step 000400): Train loss 0.954, Val loss 0.671\n",
            "Ep 2 (Step 000405): Train loss 0.544, Val loss 0.677\n",
            "Ep 2 (Step 000410): Train loss 0.647, Val loss 0.671\n",
            "Ep 2 (Step 000415): Train loss 0.848, Val loss 0.673\n",
            "Ep 2 (Step 000420): Train loss 0.862, Val loss 0.676\n",
            "Ep 2 (Step 000425): Train loss 0.781, Val loss 0.678\n",
            "Ep 2 (Step 000430): Train loss 0.303, Val loss 0.688\n",
            "Ep 2 (Step 000435): Train loss 0.933, Val loss 0.683\n",
            "Ep 2 (Step 000440): Train loss 0.520, Val loss 0.675\n",
            "Ep 2 (Step 000445): Train loss 0.744, Val loss 0.667\n",
            "Ep 2 (Step 000450): Train loss 0.423, Val loss 0.669\n",
            "Ep 2 (Step 000455): Train loss 0.403, Val loss 0.683\n",
            "Ep 2 (Step 000460): Train loss 0.676, Val loss 0.706\n",
            "Ep 2 (Step 000465): Train loss 0.218, Val loss 0.692\n",
            "Ep 2 (Step 000470): Train loss 0.205, Val loss 0.682\n",
            "Ep 2 (Step 000475): Train loss 0.474, Val loss 0.680\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: \"Analyze the input text and categorize its emotional tone as either positive or negative based on the language and context used within the given statement.\"  ### Input: proves once again he has n't lost his touch , bringing off a superb performance in an admittedly middling film .   ### Response: Positive  ### Response: Positive<|endoftext|>The following blog post, unless otherwise noted, was written by a member of Gamasutra's community.  The thoughts and opinions expressed are those of the writer and not Gamasutra or its parent\n",
            "Training completed in 5.47 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot losses"
      ],
      "metadata": {
        "id": "nHNALimXBjIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "i5oOT_cgBiP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "ddde5016-dacd-477e-9f2d-5898f838583e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate Response"
      ],
      "metadata": {
        "id": "K2uxqvP3FMvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "id": "agnW00DSFLOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf4ebf5-b794-424b-e8f7-bd9470e0a2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Extract the answer from the provided context for the question below. \"Please identify the artist responsible for the notable mural adorning the exterior of the Theodore M. Hesburgh Library.\"\n",
            "\n",
            "### Input:\n",
            "The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the signal for a touchdown.\n",
            "\n",
            "### Response:\n",
            "Millard Sheets\n",
            "\n",
            "Correct response:\n",
            ">> Millard Sheets\n",
            "\n",
            "Model response:\n",
            ">> Millard Sheets\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "\"Process the input text using Named Entity Recognition to extract and categorize all relevant entities, including but not limited to persons, organizations, and locations, into their respective categories.\"\n",
            "\n",
            "### Input:\n",
            "The figures represent a 13.6 percent increase for passenger cars and a 2.2 percent decline for trucks from July 1995 .\n",
            "\n",
            "### Response:\n",
            "No named entities detected.\n",
            "\n",
            "Correct response:\n",
            ">> No named entities detected.\n",
            "\n",
            "Model response:\n",
            ">> No named entities detected.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "\"Perform Named Entity Recognition on the given text to extract and categorize entities such as persons, organizations, locations, and other relevant information into their respective categories.\"\n",
            "\n",
            "### Input:\n",
            "German first-time registrations of motor vehicles jumped 14.2 percent in July this year from the year-earlier period , the Federal office for motor vehicles said on Thursday .\n",
            "\n",
            "### Response:\n",
            "Miscellaneous: German, Organization: Federal office for motor vehicles\n",
            "\n",
            "Correct response:\n",
            ">> Miscellaneous: German, Organization: Federal office for motor vehicles\n",
            "\n",
            "Model response:\n",
            ">> Miscellaneous: German, Organization: Federal office for motor vehicles\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving Test Data Responses"
      ],
      "metadata": {
        "id": "VLxZRi98GywR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response2.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing\n",
        "files.download(\"instruction-data-with-response2.json\")"
      ],
      "metadata": {
        "id": "I3f8s5d6GyP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ced2cef7-538f-4dd1-cae1-7b67802cf24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [04:06<00:00,  8.20s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_455b44fe-b041-455a-bf34-e11db528932c\", \"instruction-data-with-response2.json\", 42344)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_diferentes = [id for id in test_data if id['model_response'] != id['output']]\n",
        "len(ids_diferentes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8hKxfu9y6UH",
        "outputId": "4ea00567-0b7f-4908-e3da-a92957ea9632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving Model"
      ],
      "metadata": {
        "id": "8MyVtlEfHLyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n"
      ],
      "metadata": {
        "id": "IXaB6wR3HM53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dfce56-1390-4554-c22d-a7110647ee13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ]
    }
  ]
}