{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "UZDrO0Kui_Vs",
        "outputId": "cbccb32e-bec8-447a-cef1-72fb055f6d92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1186fde2-5c4b-4e80-baf6-6ed263c1f318\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1186fde2-5c4b-4e80-baf6-6ed263c1f318\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resultados_avaliacoes.json to resultados_avaliacoes.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-033266af-9e5e-4ad8-8572-f9875eb4572d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-033266af-9e5e-4ad8-8572-f9875eb4572d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving judge-small-dataset.json to judge-small-dataset.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "results = files.upload()\n",
        "results2 = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "results_medium_dataset = json.loads(results['resultados_avaliacoes.json'].decode('utf-8'))\n",
        "results_small_dataset = json.loads(results2['judge-small-dataset.json'].decode('utf-8'))"
      ],
      "metadata": {
        "id": "LEv9GxCJkKaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_medium_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qly9YQPeogp9",
        "outputId": "983d1b5a-a80e-439e-f7dd-58de3f89ae95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': {'factual_correction': 5, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provides a direct and accurate answer, following the instruction and being clear and useful'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "medium = {}\n",
        "qtd_winners_small = {}\n"
      ],
      "metadata": {
        "id": "JAWfkzGull_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small = {\n",
        "    'qtd_winners': {},\n",
        "    'A': {},\n",
        "    'B': {}\n",
        "}\n",
        "medium = {\n",
        "    'qtd_winners': {},\n",
        "    'A': {},\n",
        "    'B': {}\n",
        "}\n",
        "experiments = [(small, results_small_dataset), (medium, results_medium_dataset)]\n",
        "metrics = ['factual_correction', 'adherence', 'clarity']\n",
        "models = ['A', 'B']\n",
        "total_small = len(results_small_dataset)\n",
        "total_medium = len(results_medium_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "xYHVKeuHswK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for report, dataset in experiments:\n",
        "  for model in models:\n",
        "\n",
        "    for metric in metrics:\n",
        "        media = sum(result[model][metric] for result in dataset) / len(dataset)\n",
        "        report[model][f'media_{metric}'] = media\n",
        "\n",
        "  for result in dataset:\n",
        "    winner = str(result['Winner']['winner'])\n",
        "    if winner not in medium['qtd_winners']:\n",
        "        report['qtd_winners'][winner] = 0\n",
        "    report['qtd_winners'][winner] += 1\n",
        "print(experiments[0])\n",
        "print(experiments[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kt4cgpJtNEK",
        "outputId": "b9e2214c-476f-4dc2-d568-f3272b450194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'qtd_winners': {'B': 1, 'None': 1, 'draw': 1, 'Neither': 1, 'A': 1}, 'A': {'media_factual_correction': 0.23333333333333334, 'media_adherence': 0.26666666666666666, 'media_clarity': 0.23333333333333334}, 'B': {'media_factual_correction': 3.933333333333333, 'media_adherence': 3.933333333333333, 'media_clarity': 3.9}}, [{'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately identifies the artist responsible for the mural, adheres to the instruction, and provides a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified that there are no named entities in the input text and provided a clear and useful response, while A failed to do so'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified and categorized the entities in the text, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 5, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately categorized the emotional tone as negative, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately categorized the emotional tone as Negative, adhered to the instruction, and provided a clear and useful response, whereas A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 2, 'adherence': 1, 'clarity': 1}, 'Winner': {'winner': 'None', 'why': 'Both models provided incorrect responses, but model B had some correct locations, however neither followed the instructions correctly'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 1, 'adherence': 2, 'clarity': 1}, 'Winner': {'winner': 'B', 'why': 'Model B provided more relevant information'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly categorized the emotional tone of the input text as positive and provided a clear response, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'draw', 'why': 'neither model provided accurate or relevant responses'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'model B directly answers the question with the correct statistic, following the instruction and providing a clear response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly categorized the sentiment as positive and adhered to the instruction'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the entities and followed the instruction, while A did not'}}, {'A': {'factual_correction': 1, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'Neither', 'why': 'Neither model provided a response that accurately completed the Named Entity Recognition task as instructed.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided a correct and relevant response that adhered to the instruction, while Model A did not.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately categorized the sentiment as Positive and adhered to the instruction, while Model A failed to provide a relevant response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the sentiment as negative, following the instruction and providing a clear response, while A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and accurate answer to the question, while A provided irrelevant information'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment of the text as positive, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 1, 'adherence': 1, 'clarity': 1}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'A', 'why': 'Model A provided a more accurate and relevant response to the instruction, while Model B produced a nonsensical output'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified that there are no named entities in the input text, while Model A incorrectly identified numerous entities.'}}, {'A': {'factual_correction': 2, 'adherence': 1, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately and concisely captures the essential information from the news article, adhering to the instruction and providing clear and useful information'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately and clearly summarizes the main points of the news article, adhering to the instruction and providing useful information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'Neither', 'why': 'Both models failed to provide accurate and relevant responses.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the location as BEIJING, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 1, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and correct answer following the instruction, while A gave a redundant and partially incorrect list.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'A', 'why': 'neither model performed well, but model A at least tried to categorized the entity'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately captures the main ideas and crucial information from the text in a concise and readable overview, while A fails to provide any meaningful information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a correct sentiment label based on the input text, adhered to the instruction, and gave a clear and useful response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately distills the main ideas and key information from the news article into a concise and readable format, following the instruction completely, and providing clear and useful information.'}}, {'A': {'factual_correction': 2, 'adherence': 1, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B has better factual correction, adherence to instruction, and clarity'}}])\n",
            "({'qtd_winners': {'B': 94, 'draw': 3, 'Neither': 1, 'A': 2}, 'A': {'media_factual_correction': 0.44, 'media_adherence': 0.33, 'media_clarity': 0.34}, 'B': {'media_factual_correction': 4.79, 'media_adherence': 4.76, 'media_clarity': 4.78}}, [{'A': {'factual_correction': 5, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provides a direct and accurate answer, following the instruction and being clear and useful'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': \"B accurately identifies U.S. News & World Report as the organization that bestowed the distinction of being 'outstanding' upon the First Year of Studies program at Notre Dame, while A does not address the question and provides irrelevant information.\"}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted the main points from the news article, adhered to the instruction, and provided clear and useful information, whereas A failed to do so'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the sentiment as negative, while A incorrectly identified it as positive, and B also better adhered to the instruction and provided a clearer response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided a concise and accurate summary of the main points, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly categorized the sentimental tone as negative, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'model B provided a clear and accurate response that adhered to the instruction and correctly identified the sentiment of the input text as negative'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted key information from the original text, adhered to the instruction, and provided a clear and useful summary.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'model B accurately extracted and classified entities, followed the instruction, and provided a clear and useful response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly classified the sentiment as positive, despite the presence of words with negative connotations, demonstrating a more nuanced understanding of language and tone.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified that there are no named entities in the input text and provided the correct output as per the instruction.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly classified the sentiment as negative and provided a clear response, while Model A failed to address the instruction and provided an unclear response'}}, {'A': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'draw', 'why': 'both models correctly identified that there are no named entities in the input text'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'Neither', 'why': 'Both models failed to provide accurate named entity recognition.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': \"B correctly classified the sentiment as Positive based on the input text containing the word 'better', which indicates a positive sentiment, whereas A incorrectly classified it as Negative\"}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly followed the instruction and provided a clear response, while Model A did not.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provides a concise and coherent overview of the main ideas and essential information from the provided text, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 5, 'adherence': 1, 'clarity': 1}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided a more accurate and concise response that adhered to the instruction and was clearer and more useful than Model A'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment as negative and provided a clear response, whereas Model A failed to address the instruction and provided an unclear response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided the correct answer, adhered to the instruction, and was clear and useful.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 4, 'adherence': 4, 'clarity': 4}, 'Winner': {'winner': 'B', 'why': 'B correctly identified S&P as an organization and did not mistakenly categorize it as a person'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified that there are no named entities in the input text, following the instruction accurately and providing a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and correct answer to the question, while A did not address the question at all.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and accurate answer to the question, following the instruction and providing a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted key points, adhered to the instruction, and provided a clear and useful summary.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment polarity of the input text as positive and provided a clear and useful response, while Model A failed to provide a relevant response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted the main events, key figures, and significant details from the input text, while A failed to provide any relevant information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted the most crucial information, adhered to the instruction, and provided a clear and useful overview.'}}, {'A': {'factual_correction': 0, 'adherence': 5, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly classified the sentiment as negative, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 4, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately identified entities and followed the instruction, while Model A did not.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and correct answer to the question, while A did not'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately extracted and categorized entities, adhered to instructions, and provided clear output, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately distilled the main ideas and essential information from the news article into a concise and readable abstract, while A failed to provide any relevant information'}}, {'A': {'factual_correction': 2, 'adherence': 1, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provides a straightforward definition for the commonly used abbreviation in the context of American college athletics, following the instruction and providing a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provides the correct founding date of the marching band associated with the University of Notre Dame, adheres to the instruction, and is clear and useful.'}}, {'A': {'factual_correction': 1, 'adherence': 1, 'clarity': 1}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'A', 'why': 'Model A provided a response that somewhat followed the instruction, while Model B failed to correctly identify entities and provided repetitive and incorrect information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately condensed the main points of the input text, adhered to the instruction, and provided a clear and useful summary, whereas A failed to address the task and provided irrelevant and redundant information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately captures the main ideas and supporting details from the provided text, adheres to the instruction, and provides a clear and useful summary.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the sentiment as positive and followed the instruction, while A incorrectly identified the sentiment as negative and did not follow the instruction'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the sentiment as negative and followed the instruction accurately.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately summarized the main points of the news article, adhered to the instruction, and provided clear and useful information, while A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'Winner': {'winner': 'draw', 'why': 'both models failed to provide a reasonable response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately identified the sentiment as positive and followed the instructions correctly, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided a direct and correct answer to the question, following the instruction accurately and clearly.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and accurate response to the question, following the instruction and being clear and useful.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provides a concise and accurate summary of the input text, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provides a concise and readable abstract that accurately captures the main ideas and crucial information from the input text, while A does not follow the instruction and appears to be unrelated to the task.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a correct and direct response, while A included unnecessary repetitions.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately followed the instruction, provided a concise overview, and highlighted the most critical aspects of the text.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment as negative and followed the instruction, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentimental tone as Negative, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 2, 'adherence': 1, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provides a concise and accurate summary of the main ideas, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately summarizes the main points of the input text, adheres to the instruction, and provides clear and useful information, whereas A is irrelevant and does not meet the requirements.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided the correct answer and adhered to the instruction, while A contained incorrect information'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately extracted relevant entities and followed the instructions, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately categorized the sentiment as negative and adhered to the instruction, while A failed to do so'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and accurate answer to the question, while A provided incorrect and irrelevant information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the EU as an organization, German and British as miscellaneous, and UK as a location, while Model A provided irrelevant information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately and concisely summarizes the main points of the text, adhering to the instruction and providing clear and useful information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the organization as CME, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted the main ideas from the text, adhered to the instruction, and provided a clear and concise overview, while A failed to do so'}}, {'A': {'factual_correction': 2, 'adherence': 1, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provides a concise and accurate summary of the main ideas, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a correct and clear extraction of entities, following the instruction accurately, while A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment as Positive, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately distilled the essential information from the input text into a concise and coherent overview, highlighting the main ideas and key details, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately summarized the main points, followed the instruction, and provided a clear and useful overview, while Model A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately summarized the main points, followed the instruction, and provided a clear and useful overview'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 4, 'adherence': 4, 'clarity': 4}, 'Winner': {'winner': 'B', 'why': 'B provided more accurate and relevant information, adhered to the instruction by categorizing entities correctly, and was clearer and more useful in its response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided the correct name of the president, adhered to the instruction, and was clear and useful in its response'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 2, 'adherence': 1, 'clarity': 1}, 'Winner': {'winner': 'B', 'why': 'Model B has slightly better factual correction and more adherence to the instruction despite lacking clarity'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately and concisely summarizes the main points of the text, adhering to the instruction and providing clear information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a clear and accurate answer that adhered to the instruction, while A did not'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment as positive and followed the instruction without providing unnecessary or contradictory information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and accurate answer to the question, while A did not'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the organization and followed the instructions precisely, while A included incorrect and irrelevant information.'}}, {'A': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'draw', 'why': 'Both models correctly identified that there are no named entities in the input text and provided the same output, adhering to the instructions with equal clarity and usefulness.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately extracted the correct enrollment figure and followed the instruction precisely, providing a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately categorized the emotional tone as positive and directly addressed the instruction without unnecessary text'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately answered the question with the correct year, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately answered the question with the correct team, USC, while A provided irrelevant information.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provides a concise and accurate summary of the main points, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly classified the sentiment as Negative and adhered to the instruction, providing a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment as positive and followed the instruction accurately'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly extracted and classified the entity, while Model A failed to do so.'}}, {'A': {'factual_correction': 5, 'adherence': 1, 'clarity': 1}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided a more accurate and direct response that adhered to the instruction, was clear, and useful'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified entities and followed the instruction, while Model A had incorrect categorization.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment polarity of the input text as negative, while Model A failed to do so, providing inconsistent and incorrect labels.'}}, {'A': {'factual_correction': 5, 'adherence': 1, 'clarity': 1}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a more accurate and relevant response, following the instruction and providing a clear output.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B correctly identified the sentiment as Negative, adhered to the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 2, 'adherence': 1, 'clarity': 2}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately extracted the most important details and key takeaways, presenting them in a clear and concise manner, fully adhering to the instruction.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': \"B provided the correct number of doctoral and master's degree programs and followed the instruction accurately.\"}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B provided a direct and accurate answer to the question, with no irrelevant information.'}}, {'A': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'B': {'factual_correction': 5, 'adherence': 2, 'clarity': 4}, 'Winner': {'winner': 'A', 'why': 'Model A provided a clear and direct response that accurately answered the question, following the instruction and providing useful information.'}}, {'A': {'factual_correction': 0, 'adherence': 5, 'clarity': 5}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B correctly identified the sentiment as negative, which aligns with the overall emotional tone conveyed by the language used'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately distilled the main points from the text into a concise and informative abstract, focusing on key events, figures, and context, while A failed to do so.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'B accurately categorized the sentiment as positive and provided a clear and useful response, whereas A incorrectly categorized the sentiment as negative and provided an unclear response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provides a concise and accurate summary of the article, adhering to the instruction and presenting the information in a clear and useful manner.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately answered the question with the correct number of student-run newspapers, followed the instruction, and provided a clear and useful response.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B accurately extracted key points from the text, adhered to the instruction, and provided a clear and useful summary.'}}, {'A': {'factual_correction': 0, 'adherence': 0, 'clarity': 0}, 'B': {'factual_correction': 5, 'adherence': 5, 'clarity': 5}, 'Winner': {'winner': 'B', 'why': 'Model B provided a concise and accurate summary of the input text, adhering to the instruction and presenting the information in a clear and useful manner.'}}])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small = {\n",
        "    'qtd_winners': {},\n",
        "    'A': {},\n",
        "    'B': {}\n",
        "}\n",
        "metrics = ['factual_correction', 'adherence', 'clarity']\n",
        "models = ['A', 'B']\n",
        "\n",
        "total = len(results_small_dataset)\n",
        "\n",
        "for model in models:\n",
        "    for metric in metrics:\n",
        "        media = sum(result[model][metric] for result in results_small_dataset) / total\n",
        "        small[model][f'media_{metric}'] = media\n",
        "\n",
        "for result in results_small_dataset:\n",
        "    winner = str(result['Winner']['winner'])\n",
        "    if winner not in small['qtd_winners']:\n",
        "        small['qtd_winners'][winner] = 0\n",
        "    small['qtd_winners'][winner] += 1\n",
        "\n",
        "print(small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fBzI7Enz0u",
        "outputId": "93e09fa0-154c-4025-f3d1-c8153646e3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'qtd_winners': {'B': 24, 'None': 1, 'draw': 1, 'Neither': 2, 'A': 2}, 'A': {'media_factual_correction': 0.23333333333333334, 'media_adherence': 0.26666666666666666, 'media_clarity': 0.23333333333333334}, 'B': {'media_factual_correction': 3.933333333333333, 'media_adherence': 3.933333333333333, 'media_clarity': 3.9}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "medium = {\n",
        "    'qtd_winners': {},\n",
        "    'A': {},\n",
        "    'B': {}\n",
        "}\n",
        "metrics = ['factual_correction', 'adherence', 'clarity']\n",
        "models = ['A', 'B']\n",
        "\n",
        "total = len(results_medium_dataset)\n",
        "\n",
        "for model in models:\n",
        "    for metric in metrics:\n",
        "        media = sum(result[model][metric] for result in results_medium_dataset) / total\n",
        "        medium[model][f'media_{metric}'] = media\n",
        "\n",
        "for result in results_medium_dataset:\n",
        "    winner = str(result['Winner']['winner'])\n",
        "    if winner not in medium['qtd_winners']:\n",
        "        medium['qtd_winners'][winner] = 0\n",
        "    medium['qtd_winners'][winner] += 1\n",
        "\n",
        "print(medium)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hjvjwU8qxan",
        "outputId": "be6a3dcf-c62d-470d-fdd2-21a82e9f66f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'qtd_winners': {'B': 94, 'draw': 3, 'Neither': 1, 'A': 2}, 'A': {'media_factual_correction': 0.44, 'media_adherence': 0.33, 'media_clarity': 0.34}, 'B': {'media_factual_correction': 4.79, 'media_adherence': 4.76, 'media_clarity': 4.78}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def generate_summary(results_dataset):\n",
        "    metrics = ['factual_correction', 'adherence', 'clarity']\n",
        "    models = ['A', 'B']\n",
        "\n",
        "    summary = {\n",
        "        'qtd_winners': defaultdict(int),\n",
        "        'A': {},\n",
        "        'B': {}\n",
        "    }\n",
        "\n",
        "    total = len(results_dataset)\n",
        "\n",
        "    #  Mdias\n",
        "    for model in models:\n",
        "        for metric in metrics:\n",
        "            media = sum(r[model][metric] for r in results_dataset) / total\n",
        "            summary[model][f'media_{metric}'] = media\n",
        "\n",
        "    #  Contagem de vencedores\n",
        "    for r in results_dataset:\n",
        "        winner = str(r['Winner']['winner'])\n",
        "        summary['qtd_winners'][winner] += 1\n",
        "\n",
        "    summary['qtd_winners'] = dict(summary['qtd_winners'])\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "iIFcR41FCOR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small = generate_summary(results_small_dataset)\n",
        "medium = generate_summary(results_medium_dataset)\n",
        "\n",
        "print(\"Small:\", small)\n",
        "print(\"Medium:\", medium)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBEoLULXCPJO",
        "outputId": "bd33b86d-40c0-480d-f9b6-d3a9dc169e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small: {'qtd_winners': {'B': 24, 'None': 1, 'draw': 1, 'Neither': 2, 'A': 2}, 'A': {'media_factual_correction': 0.23333333333333334, 'media_adherence': 0.26666666666666666, 'media_clarity': 0.23333333333333334}, 'B': {'media_factual_correction': 3.933333333333333, 'media_adherence': 3.933333333333333, 'media_clarity': 3.9}}\n",
            "Medium: {'qtd_winners': {'B': 94, 'draw': 3, 'Neither': 1, 'A': 2}, 'A': {'media_factual_correction': 0.44, 'media_adherence': 0.33, 'media_clarity': 0.34}, 'B': {'media_factual_correction': 4.79, 'media_adherence': 4.76, 'media_clarity': 4.78}}\n"
          ]
        }
      ]
    }
  ]
}